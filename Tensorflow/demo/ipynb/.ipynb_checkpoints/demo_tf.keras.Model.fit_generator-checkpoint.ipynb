{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras.Model.fit_generator训练范例\n",
    "\n",
    "## 实验目的    \n",
    "* 掌握 keras.preprocessing.image.ImageDataGenerator图像数据生成器使用方法\n",
    "* 掌握model.fit_generator 及 model.predict_generator进行训练与测试的方式方法\n",
    "* 掌握callback使用方法\n",
    "* 通过软链接输出预测结果，便于观察模型性能\n",
    "\n",
    "## Tensorboard\n",
    "    参考：https://blog.csdn.net/dugudaibo/article/details/77961836\n",
    "\n",
    "    一、引入Tensorboard\n",
    "        from keras.callbacks import TensorBoard\n",
    "\n",
    "    二、设置fit回调函数，并设置日志目录\n",
    "        model.fit(x_train, y_train,\n",
    "              epochs=20,\n",
    "              batch_size=128,\n",
    "              callbacks=[TensorBoard(log_dir='tb/classes_mlp')])\n",
    "\n",
    "    三、启动Tensorboard\n",
    "        tensorboard --logdir=tb/classes_mlp\n",
    "\n",
    "    四、浏览器打开Tensorboard\n",
    "        http://localhost:6006\n",
    "\n",
    "\n",
    "## 实验数据说明\n",
    "    * 实验数据根目录：/to/path/img_classify\n",
    "      ./train/class1/\n",
    "      ......\n",
    "      ./train/classN/\n",
    "      ./valid/class1/\n",
    "      ......\n",
    "      ./valid/classN/\n",
    "      ./test/class1/\n",
    "      ......\n",
    "      ./test/classN/\n",
    "     \n",
    "## 相关函数接口说明\n",
    "### model.compile\n",
    "compile\n",
    "compile(self, optimizer, loss, metrics=[], loss_weights=None, sample_weight_mode=None)\n",
    "本函数编译模型以供训练，参数有\n",
    "* optimizer：优化器，为预定义优化器名或优化器对象，参考优化器\n",
    "* loss：目标函数，为预定义损失函数名或一个目标函数，参考目标函数\n",
    "* metrics：列表，包含评估模型在训练和测试时的性能的指标，典型用法是metrics=['accuracy']如果要在多输出模型中为不同的输出指定不同的指标，可像该参数传递一个字典，例如metrics={'ouput_a': 'accuracy'}\n",
    "* sample_weight_mode：如果你需要按时间步为样本赋权（2D权矩阵），将该值设为“temporal”。默认为“None”，代表按样本赋权（1D权）。如果模型有多个输出，可以向该参数传入指定sample_weight_mode的字典或列表。在下面fit函数的解释中有相关的参考内容。\n",
    "* kwargs：使用TensorFlow作为后端请忽略该参数，若使用Theano作为后端，kwargs的值将会传递给 K.function\n",
    "\n",
    "【Tips】如果你只是载入模型并利用其predict，可以不用进行compile。在Keras中，compile主要完成损失函数和优化器的一些配置，是为训练服务的。predict会在内部进行符号函数的编译工作（通过调用_make_predict_function生成函数）【@白菜，@我是小将】\n",
    "\n",
    "### model.fit\n",
    "fit\n",
    "fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None)\n",
    "本函数用以训练模型，参数有：\n",
    "*    x：输入数据。如果模型只有一个输入，那么x的类型是numpy array，如果模型有多个输入，那么x的类型应当为list，list的元素是对应于各个输入的numpy array。如果模型的每个输入都有名字，则可以传入一个字典，将输入名与其输入数据对应起来。\n",
    "*    y：标签，numpy array。如果模型有多个输出，可以传入一个numpy array的list。如果模型的输出拥有名字，则可以传入一个字典，将输出名与其标签对应起来。\n",
    "*    batch_size：整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步。\n",
    "*    nb_epoch：整数，训练的轮数，训练数据将会被遍历nb_epoch次。Keras中nb开头的变量均为\"number of\"的意思\n",
    "*    verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n",
    "*    callbacks：list，其中的元素是keras.callbacks.Callback的对象。这个list中的回调函数将会在训练过程中的适当时机被调用，参考回调函数\n",
    "*    validation_split：0~1之间的浮点数，用来指定训练集的一定比例数据作为验证集。验证集将不参与训练，并在每个epoch结束后测试的模型的指标，如损失函数、精确度等。\n",
    "*    validation_data：形式为（X，y）或（X，y，sample_weights）的tuple，是指定的验证集。此参数将覆盖validation_spilt。\n",
    "*    shuffle：布尔值，表示是否在训练过程中每个epoch前随机打乱输入样本的顺序。\n",
    "*    class_weight：字典，将不同的类别映射为不同的权值，该参数用来在训练过程中调整损失函数（只能用于训练）。该参数在处理非平衡的训练数据（某些类的训练样本数很少）时，可以使得损失函数对样本数不足的数据更加关注。\n",
    "*    sample_weight：权值的numpy array，用于在训练时调整损失函数（仅用于训练）。可以传递一个1D的与样本等长的向量用于对样本进行1对1的加权，或者在面对时序数据时，传递一个的形式为（samples，sequence_length）的矩阵来为每个时间步上的样本赋不同的权。这种情况下请确定在编译模型时添加了sample_weight_mode='temporal'。\n",
    "\n",
    "fit函数返回一个History的对象，其History.history属性记录了损失函数和其他指标的数值随epoch变化的情况，如果有验证集的话，也包含了验证集的这些指标变化情况\n",
    "\n",
    "### model.fit_generator\n",
    "fit_generator\n",
    "fit_generator(self, generator, samples_per_epoch, nb_epoch, verbose=1, callbacks=[], validation_data=None, nb_val_samples=None, class_weight={}, max_q_size=10)\n",
    "\n",
    "利用Python的生成器，逐个生成数据的batch并进行训练。生成器与模型将并行执行以提高效率。例如，该函数允许我们在CPU上进行实时的数据提升，同时在GPU上进行模型训练\n",
    "\n",
    "函数的参数是：\n",
    "*    generator：生成器函数，生成器的输出应该为：\n",
    "        一个形如（inputs，targets）的tuple\n",
    "        一个形如（inputs, targets,sample_weight）的tuple。所有的返回值都应该包含相同数目的样本。生成器将无限在数据集上循环。每个epoch以经过模型的样本数达到samples_per_epoch时，记一个epoch结束\n",
    "*    samples_per_epoch：整数，当模型处理的样本达到此数目时计一个epoch结束，执行下一个epoch\n",
    "*    verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n",
    "*    validation_data：具有以下三种形式之一\n",
    "        生成验证集的生成器\n",
    "        一个形如（inputs,targets）的tuple\n",
    "        一个形如（inputs,targets，sample_weights）的tuple\n",
    "*    nb_val_samples：仅当validation_data是生成器时使用，用以限制在每个epoch结束时用来验证模型的验证集样本数，功能类似于samples_per_epoch\n",
    "*    max_q_size：生成器队列的最大容量\n",
    "\n",
    "函数返回一个History对象\n",
    "\n",
    "例子\n",
    "\n",
    "def generate_arrays_from_file(path):\n",
    "    while 1:\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        # create numpy arrays of input data\n",
    "        # and labels, from each line in the file\n",
    "        x, y = process_line(line)\n",
    "        yield (x, y)\n",
    "    f.close()\n",
    "\n",
    "model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
    "        samples_per_epoch=10000, nb_epoch=10)\n",
    "\n",
    "\n",
    "### model.evaluate\n",
    "evaluate\n",
    "evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "本函数按batch计算在某些输入数据上模型的误差，其参数有：\n",
    "*    x：输入数据，与fit一样，是numpy array或numpy array的list\n",
    "*    y：标签，numpy array\n",
    "*    batch_size：整数，含义同fit的同名参数\n",
    "*    verbose：含义同fit的同名参数，但只能取0或1\n",
    "*    sample_weight：numpy array，含义同fit的同名参数\n",
    "\n",
    "本函数返回一个测试误差的标量值（如果模型没有其他评价指标），或一个标量的list（如果模型还有其他的评价指标）。model.metrics_names将给出list中各个值的含义。\n",
    "\n",
    "如果没有特殊说明，以下函数的参数均保持与fit的同名参数相同的含义\n",
    "\n",
    "如果没有特殊说明，以下函数的verbose参数（如果有）均只能取0或1\n",
    "\n",
    "### model.evaluate_generator\n",
    "evaluate_generator\n",
    "evaluate_generator(self, generator, val_samples, max_q_size=10)\n",
    "\n",
    "本函数使用一个生成器作为数据源，来评估模型，生成器应返回与test_on_batch的输入数据相同类型的数据。\n",
    "\n",
    "函数的参数是：\n",
    "*    generator：生成输入batch数据的生成器\n",
    "*    val_samples：生成器应该返回的总样本数\n",
    "*    max_q_size：生成器队列的最大容量\n",
    "*    nb_worker：使用基于进程的多线程处理时的进程数\n",
    "*    pickle_safe：若设置为True，则使用基于进程的线程。注意因为它的实现依赖于多进程处理，不可传递不可pickle的参数到生成器中，因为它们不能轻易的传递到子进程中。\n",
    "\n",
    "\n",
    "### model.predict\n",
    "predict\n",
    "predict(self, x, batch_size=32, verbose=0)\n",
    "本函数按batch获得输入数据对应的输出，其参数有：\n",
    "函数的返回值是预测值的numpy array\n",
    "\n",
    "### model.predict_generator\n",
    "predict_generator\n",
    "predict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n",
    "\n",
    "从一个生成器上获取数据并进行预测，生成器应返回与predict_on_batch输入类似的数据\n",
    "\n",
    "函数的参数是：\n",
    "*    generator：生成输入batch数据的生成器\n",
    "*    val_samples：生成器应该返回的总样本数\n",
    "*    max_q_size：生成器队列的最大容量\n",
    "*    nb_worker：使用基于进程的多线程处理时的进程数\n",
    "*    pickle_safe：若设置为True，则使用基于进程的线程。注意因为它的实现依赖于多进程处理，不可传递不可pickle的参数到生成器中，因为它们不能轻易的传递到子进程中。\n",
    "\n",
    "\n",
    "\n",
    "## 参考资料\n",
    "visualization of filters keras 基于Keras的卷积神经网络（CNN）可视化\n",
    "\n",
    "http://www.cnblogs.com/bnuvincent/p/9612686.html\n",
    "\n",
    "python深度学习{eep learning with python中文版.pdf}源码\n",
    "\n",
    "https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "\n",
    "数据下载：\n",
    "\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "\n",
    "本地数据\n",
    "\n",
    "~/e/dataset_tiptical/cats_and_dogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path='%s/work/data/gtest/classify'%os.getenv('HOME')#数据目录\n",
    "train_dir='%s/train'%data_path    #训练目录\n",
    "valid_dir='%s/valid'%data_path    #校验目录\n",
    "test_dir ='%s/test'%data_path     #测试目录\n",
    "\n",
    "out_dir  ='%s/work/temp/fit_generator'%os.getenv('HOME')   #输出目录\n",
    "log_dir  ='%s/log_dir'%out_dir    #输出日志目录\n",
    "preds_dir='%s/predicts'%out_dir   #预测结果\n",
    "cp_file='%s/cp_file.h5'%out_dir   #训练断点\n",
    "model_file='%s/model.h5'%out_dir  #模型文件\n",
    "\n",
    "input_shape=(224,224,3)\n",
    "target_size=(224,224)\n",
    "epochs=3\n",
    "num_class=10\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n",
      "Found 1000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.backend as K\n",
    "\n",
    "#构造图像数据生成器:train\n",
    "gen_train = ImageDataGenerator(\n",
    "        featurewise_center           =False,\n",
    "        samplewise_center            =False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization =False,\n",
    "        zca_whitening                =False,\n",
    "        zca_epsilon                  =1e-6,\n",
    "        rotation_range               =0.,\n",
    "        width_shift_range            =0.,\n",
    "        height_shift_range           =0.,\n",
    "        shear_range                  =0.,\n",
    "        zoom_range                   =0.,\n",
    "        channel_shift_range          =0.,\n",
    "        fill_mode                    ='nearest',\n",
    "        cval                         =0.,\n",
    "        horizontal_flip              =False,\n",
    "        vertical_flip                =False,\n",
    "        rescale                      =1./255,\n",
    "        preprocessing_function       =None,\n",
    "        data_format                  =K.image_data_format()\n",
    "       )\n",
    "data_train=gen_train.flow_from_directory(directory='%s/train'%(data_path)\n",
    "                                         ,batch_size=batch_size\n",
    "                                         ,target_size=target_size)\n",
    "#构造图像数据生成器:valid\n",
    "gen_valid = ImageDataGenerator(\n",
    "        featurewise_center           =False,\n",
    "        samplewise_center            =False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization =False,\n",
    "        zca_whitening                =False,\n",
    "        zca_epsilon                  =1e-6,\n",
    "        rotation_range               =0.,\n",
    "        width_shift_range            =0.,\n",
    "        height_shift_range           =0.,\n",
    "        shear_range                  =0.,\n",
    "        zoom_range                   =0.,\n",
    "        channel_shift_range          =0.,\n",
    "        fill_mode                    ='nearest',\n",
    "        cval                         =0.,\n",
    "        horizontal_flip              =False,\n",
    "        vertical_flip                =False,\n",
    "        rescale                      =1./255,\n",
    "        preprocessing_function       =None,\n",
    "        data_format                  =K.image_data_format()\n",
    "       )\n",
    "data_valid=gen_valid.flow_from_directory(directory='%s/valid'%(data_path)\n",
    "                                         ,batch_size=batch_size\n",
    "                                         ,target_size=target_size)\n",
    "\n",
    "#构造图像数据生成器:test\n",
    "gen_test = ImageDataGenerator(\n",
    "        featurewise_center           =False,\n",
    "        samplewise_center            =False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization =False,\n",
    "        zca_whitening                =False,\n",
    "        zca_epsilon                  =1e-6,\n",
    "        rotation_range               =0.,\n",
    "        width_shift_range            =0.,\n",
    "        height_shift_range           =0.,\n",
    "        shear_range                  =0.,\n",
    "        zoom_range                   =0.,\n",
    "        channel_shift_range          =0.,\n",
    "        fill_mode                    ='nearest',\n",
    "        cval                         =0.,\n",
    "        horizontal_flip              =False,\n",
    "        vertical_flip                =False,\n",
    "        rescale                      =1./255,\n",
    "        preprocessing_function       =None,\n",
    "        data_format                  =K.image_data_format()\n",
    "       )\n",
    "data_test=gen_test.flow_from_directory(directory='%s/test'%(data_path)\n",
    "                                       ,batch_size=batch_size\n",
    "                                       ,shuffle=False                                       \n",
    "                                       ,target_size=target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'poly8': 8, 'poly9': 9, 'poly4': 4, 'line': 2, 'poly5': 5, 'ellipse': 1, 'poly6': 6, 'poly7': 7, 'circle': 0, 'poly3': 3}\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9]\n"
     ]
    }
   ],
   "source": [
    "#data_test.filenames\n",
    "print(data_test.class_indices)\n",
    "print(data_test.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 394272)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               201867776 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 201,873,802\n",
      "Trainable params: 201,873,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models,layers,optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=input_shape,name='conv2d_1'))\n",
    "model.add(layers.MaxPooling2D((2, 2),name='max_pooling2d_1'))\n",
    "model.add(layers.Flatten(name='flatten_1'))\n",
    "model.add(layers.Dense(512, activation='relu',name='dense_1'))\n",
    "model.add(layers.Dense(num_class, activation='softmax',name='dense_2'))\n",
    "\n",
    "#打印模型\n",
    "model.summary()\n",
    "\n",
    "#模型编译\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "          metrics=['acc'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网路训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training beginning ......\n",
      "Epoch 1/3\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.1400 - acc: 0.6513 - val_loss: 1.7079 - val_acc: 0.4784\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 6s 194ms/step - loss: 0.7020 - acc: 0.7861 - val_loss: 1.6800 - val_acc: 0.4871\n",
      "\n",
      "Epoch 00002: val_acc improved from -inf to 0.48711, saving model to /home/hjw/work/temp/fit_generator/cp_file.h5\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 6s 202ms/step - loss: 0.4973 - acc: 0.8564 - val_loss: 1.6219 - val_acc: 0.5292\n",
      "history: {'acc': [0.649, 0.784, 0.856], 'val_acc': [0.4784263959390863, 0.48711340206185566, 0.5291878172588832], 'val_loss': [1.707866078100834, 1.680048615662093, 1.6218553860175429], 'loss': [1.1472428169250488, 0.7046870613098144, 0.5008905379772186]}\n"
     ]
    }
   ],
   "source": [
    "#模型训练\n",
    "print('training beginning ......')\n",
    "\n",
    "#断点加载\n",
    "if os.path.exists(cp_file):\n",
    "    model.load_weights(cp_file)\n",
    "            \n",
    "#断点训练:monitor监控参数可以通过score = model.evaluate(x_test, y_test, verbose=0)的score查询\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(cp_file, monitor='val_acc', verbose=1, save_best_only=True, mode='auto',period=2)\n",
    "#EarlyStopping\n",
    "earlyStopping_cb=keras.callbacks.EarlyStopping(monitor='acc', patience=3, verbose=0, mode='max')\n",
    "#TensorBoard\n",
    "tensorBoard_cb=keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "#回调函数序列\n",
    "callbacks_list = [checkpoint_cb,earlyStopping_cb,tensorBoard_cb]\n",
    "\n",
    "#模型训练\n",
    "history = model.fit_generator(\n",
    "  data_train,\n",
    "  steps_per_epoch=np.ceil(data_train.samples/batch_size),\n",
    "  epochs=epochs,\n",
    "  validation_data=data_valid,\n",
    "  validation_steps=50,\n",
    "  callbacks=callbacks_list)\n",
    "print('history:',history.history)\n",
    "\n",
    "#保存模型\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting beginning ......\n",
      "32/32 [==============================] - 2s 60ms/step\n",
      "samples: 1000\n",
      "classes[:2]:\n",
      "[0 0]\n",
      "y_pred.shape: (1000, 10)\n",
      "y_pred[:2]:\n",
      "[[6.65797174e-01 2.20608730e-02 4.46087142e-05 1.65605758e-04\n",
      "  2.87782121e-03 2.01697554e-02 1.03531443e-02 3.24518420e-02\n",
      "  2.24697292e-01 2.13818196e-02]\n",
      " [2.37327412e-01 1.07521348e-01 1.31195784e-02 1.15222586e-02\n",
      "  8.71749781e-03 9.05097872e-02 2.52631634e-01 1.23366654e-01\n",
      "  2.23483387e-02 1.32935420e-01]]\n",
      "准确率: 0.499\n"
     ]
    }
   ],
   "source": [
    "from mylibs.predicts_to_symlink import predicts_to_symlink\n",
    "#计算精度\n",
    "def compute_acc(y_pred,y_true):\n",
    "    acc=(y_pred-y_true)==0\n",
    "    return acc.sum()/acc.size\n",
    "\n",
    "#加载模型\n",
    "model.load_weights(model_file)\n",
    "\n",
    "#模型测试\n",
    "print('predicting beginning ......')\n",
    "#type(y_pred)=> <class 'numpy.ndarray'>\n",
    "y_pred=model.predict_generator(\n",
    "    data_test, \n",
    "    steps=None, #预测轮数\n",
    "    max_queue_size=32, \n",
    "    workers=1, \n",
    "    use_multiprocessing=False, \n",
    "    verbose=1)\n",
    "\n",
    "#输出软链接目录\n",
    "predicts_to_symlink(y_pred,test_dir,preds_dir,data_test)\n",
    "\n",
    "#准确率计算\n",
    "acc=compute_acc(np.argmax(y_pred,axis=1),data_test.classes)\n",
    "print('samples:',data_test.samples)\n",
    "print('classes[:2]:')\n",
    "print(data_test.classes[:2])\n",
    "print('y_pred.shape:',y_pred.shape)\n",
    "print('y_pred[:2]:')\n",
    "print(y_pred[:2])\n",
    "print('准确率:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
