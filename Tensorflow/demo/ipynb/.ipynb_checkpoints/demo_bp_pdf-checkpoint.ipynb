{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP算法\n",
    "    BP算法详细描述，参考韩力群的《人工神经网络教程》P58 - P64\n",
    "  \n",
    "#### 基于BP算法的多层感知器模型\n",
    "三层BP网络\n",
    "\n",
    "输入向量：X=$(x_{1},x_{2},...,x_{i},...,x_{n})^T$ ,$x_{0}=-1$是为隐层神经元引入阈值而设置.\n",
    "\n",
    "隐层输出向量:Y=$(y_{1},y_{2},...,y_{j},...,y_{m})^T$ ,$y_{0}=-1$是为输出层神经元引入阈值而设置.\n",
    "\n",
    "输出层输出向量:O=$(o_{1},o_{2},...,o_{k},...,o_{l})^T$.\n",
    "\n",
    "输入层到隐层之间的权值矩阵用V表示，V=$(V_{1},V_{2},...,V_{j},...,V_{m})$.其中列向量$V_{j}$为隐层第j个神经元对应的权向量.\n",
    "\n",
    "隐层到输出层之间的权值矩阵用W表示，V=$(W_{1},W_{2},...,W_{k},...,W_{l})$.其中列向量$W_{k}$为输出层第k个神经元对应的权向量.\n",
    "\n",
    "各层之间的数学关系：\n",
    "\n",
    "对于输出层，有\n",
    "\n",
    "$o_{k}=f(net_{k})   \\qquad\\qquad  k=1,2,...,l $     $\\qquad$(3.10)\n",
    "\n",
    "$net_{k}=\\sum_{j=0}^mw_{jk}y_{j}\\qquad  k=1,2,...,l $ $\\qquad$ (3.11)\n",
    "\n",
    "对于隐层，有\n",
    "\n",
    "$y_{j}=f(net_{j})  \\qquad\\qquad j=1,2,...,m $     $\\qquad$ (3.12)\n",
    "\n",
    "$net_{j}=\\sum_{i=0}^nv_{ij}x_{i} \\qquad j=1,2,...,m$  $\\qquad$ (3.13)\n",
    "\n",
    "变换函数为Sigmoid函数\n",
    "\n",
    "$f(x)=\\frac{1}{1+e^{-x}}$   $\\qquad$ (3.14)\n",
    "\n",
    "$f'(x)=f(x)(1-f(x))$    $\\qquad$ (3.15)\n",
    "\n",
    "#### BP学习算法\n",
    "    下面以三层感知器为例介绍BP学习算法\n",
    "* 网络误差与权值调整\n",
    "\n",
    "当网络输出与期望输出不等时，存在输出误差E,定义如下：\n",
    "\n",
    "$E=\\frac{1}{2}(d-o)^2=\\frac{1}{2}\\sum_{k=1}^l(d_{k}-o_{k})^2$  $\\qquad$  (3.16)\n",
    "\n",
    "将以上定义展开至隐层\n",
    "\n",
    "$E=\\frac{1}{2}(d-o)^2=\\frac{1}{2}\\sum_{k=1}^l(d_{k}-o_{k})^2 \\\\\n",
    "=\\frac{1}{2}\\sum_{k=1}^l[d_{k}-f(net_{k})]^2 \\\\\n",
    "=\\frac{1}{2}\\sum_{k=1}^l[d_{k}-f(\\sum_{j=0}^mw_{jk}y_{j})]^2 \\\\\n",
    "$                                                                $\\qquad$ (3.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步展开至输入层\n",
    "\n",
    "$E=\\frac{1}{2}(d-o)^2=\\frac{1}{2}\\sum_{k=1}^l(d_{k}-o_{k})^2 \\\\\n",
    "=\\frac{1}{2}\\sum_{k=1}^l[d_{k}-f(net_{k})]^2 \\\\\n",
    "=\\frac{1}{2}\\sum_{k=1}^l[d_{k}-f(\\sum_{j=0}^mw_{jk}y_{j})]^2 \\\\\n",
    "=\\frac{1}{2}\\sum_{k=1}^l[d_{k}-f(\\sum_{j=0}^mw_{jk}f(net_{j}))]^2 \\\\\n",
    "=\\frac{1}{2}\\sum_{k=1}^l[d_{k}-f(\\sum_{j=0}^mw_{jk}f(\\sum_{i=0}^nv_{ij}x_{i}))]^2 \\\\\n",
    "$                                                                $\\qquad $(3.18)\n",
    "\n",
    "由上式可以看出，网络输入误差是各层权值$v_{ij},w_{jk}$的函数，因此调整权值可以改变误差E.\n",
    "\n",
    "显然，调整权值的原则是使误差不断减少，因此应使权值的调整量与误差的梯度下降成正比，即\n",
    "\n",
    "\n",
    "$\\Delta w_{jk}=-\\eta \\frac{\\partial E}{\\partial w_{jk}} \\qquad j=0,1,2,...,m \\qquad k=1,2,...,l$  $\\qquad$ (3.19a)\n",
    "\n",
    "$\\Delta v_{ij}=-\\eta \\frac{\\partial E}{\\partial v_{jk}} \\qquad i=0,1,2,...,n \\qquad j=1,2,...,m$  $\\qquad$ (3.19b)\n",
    "\n",
    "式中符号表示梯度下降，常数$\\eta$表示学习率 [0,1]\n",
    "\n",
    "* BP算法推导\n",
    "\n",
    "$\\Delta w_{jk}=-\\eta \\frac{\\partial E}{\\partial w_{jk}}\n",
    "=-\\eta \\frac{\\partial E}{\\partial net_{k}} \\frac{\\partial net_{k}}{\\partial w_{jk}}$   $\\qquad$ (3.20a)\n",
    "\n",
    "$\\Delta v_{ij}=-\\eta \\frac{\\partial E}{\\partial v_{ij}}\n",
    "=-\\eta \\frac{\\partial E}{\\partial net_{j}} \\frac{\\partial net_{j}}{\\partial v_{ij}}$   $\\qquad$ (3.20b)\n",
    "\n",
    "定义误差信号，令\n",
    "\n",
    "$\\delta ^o_{k}=-\\frac{\\partial E}{\\partial net_{k}}$   $\\qquad$ (3.21a)\n",
    "\n",
    "$\\delta ^y_{j}=-\\frac{\\partial E}{\\partial net_{j}}$   $\\qquad$ (3.21b)\n",
    "\n",
    "则\n",
    "\n",
    "$\\Delta w_{jk}=\\eta \\delta^o_{k} y_{j}$    $\\qquad$ (3.22a)\n",
    "\n",
    "$\\Delta v_{ij}=\\eta \\delta^y_{j} x_{i}$    $\\qquad$ (3.22b)\n",
    "\n",
    "展开$\\delta$\n",
    "\n",
    "$\\delta ^o_{k}=-\\frac{\\partial E}{\\partial net_{k}}\n",
    "=-\\frac{\\partial E}{\\partial o_{k}} \\frac{\\partial o_{k}}{\\partial net_{k}}\n",
    "=-\\frac{\\partial E}{\\partial o_{k}} f'(net_{k})$    $\\qquad$ (3.23a)\n",
    "\n",
    "$\\delta ^y_{j}=-\\frac{\\partial E}{\\partial net_{j}}\n",
    "=-\\frac{\\partial E}{\\partial y_{j}} \\frac{\\partial y_{j}}{\\partial net_{j}}\n",
    "=-\\frac{\\partial E}{\\partial y_{j}} f'(net_{j})$    $\\qquad$ (3.23b) \n",
    "\n",
    "$\\frac{\\partial E}{\\partial o_{k}}=-(d_{k}-o_{k})$  $\\qquad$ (3.24a)  \n",
    "\n",
    "$\\frac{\\partial E}{\\partial y_{j}}\n",
    "=-\\sum_{k=1}^l(d_{k}-o_{k})f'(net_{k})w_{jk}$        $\\qquad$ (3.24b)\n",
    "\n",
    "$\\delta ^o_{k}=-\\frac{\\partial E}{\\partial net_{k}}\n",
    "=(d_{k}-o_{k})o_{k}(1-o_{k})\n",
    "$                                                    $\\qquad$ (3.25a)\n",
    "\n",
    "$\\delta ^y_{j}=-\\frac{\\partial E}{\\partial net_{j}}\n",
    "=[\\sum_{k=1}^l(d_{k}-o_{k})f'(net_{k})w_{jk}]f'(net_{j})\n",
    "=(\\sum_{k=1}^l\\delta ^o_{k}w_{jk})y_{j}(1-y_{j})\n",
    "$                                                    $\\qquad$ (3.25b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "权重调整公式\n",
    "\n",
    "$\\Delta w_{jk}=\\eta \\delta^o_{k} y_{j}=\\eta (d_{k}-o_{k})o_{k}(1-o_{k}) y_{j}$    $\\qquad$ (3.26a)\n",
    "\n",
    "$\\Delta v_{ij}=\\eta \\delta^y_{j} x_{i}=\\eta (\\sum_{k=1}^l\\delta ^o_{k}w_{jk})y_{j}(1-y_{j}) x_{i}$    $\\qquad$ (3.26b)\n",
    "\n",
    "#### BP算法的程序实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#代码中使用变量/函数解析\n",
    "#P  #样本数\n",
    "#n  #输入向量长度\n",
    "#m  #隐藏层神经元个数\n",
    "#l  #输出向量长度\n",
    "\n",
    "#x  #输入矩阵       P x n\n",
    "#y  #隐层输出矩阵    P x m\n",
    "#o  #输出层输出矩阵  P x l\n",
    "#d  #期望输出值矩阵  P x l\n",
    "#f  #变换函数\n",
    "\n",
    "#v  #输入到隐层之间的权值矩阵  m x n\n",
    "#w  #隐层到输出之间的权值矩阵  l x m\n",
    "\n",
    "#err_total #计算总误差\n",
    "\n",
    "P=1\n",
    "n=2\n",
    "m=2\n",
    "l=2\n",
    "lamda=1.0 #学习率\n",
    "epochs=20\n",
    "\n",
    "#构造训练样本\n",
    "#x=np.array([[0.2,1.2]],dtype=np.float32)         #输入向量\n",
    "#d=np.array([[0.4,0.3,0.2,0.1]],dtype=np.float32) #期望值\n",
    "x=np.random.rand(P,n) #输入矩阵 ,[P x n]\n",
    "d=np.random.rand(P,l) #期望矩阵 ,[P x l]\n",
    "P=d.shape[0]          #样本数\n",
    "n=x.shape[1]          #输入向量长度\n",
    "l=d.shape[1]          #输出向量长度\n",
    "\n",
    "#初始化V,W\n",
    "v=np.random.rand(m,n)  # m x n\n",
    "w=np.random.rand(l,m)  # l x m\n",
    "\n",
    "print('x.shape:%s,d.shape:%s,v.shape:%s,w.shape:%s'%(x.shape,d.shape,v.shape,w.shape))\n",
    "print('P:%s,n:%s,m:%s,l:%s'%(P,n,m,l))\n",
    "\n",
    "#sigmoid function\n",
    "def f(x, deriv = False):\n",
    "    if (deriv == True):\n",
    "        return x * (1 - x)       #如果deriv为true,求导数,此时的x为节点输出值，即f(x)\n",
    "    return 1 / (1 + np.exp(-x))  #exp()是以e为底的指数函数\n",
    "\n",
    "#计算总误差\n",
    "def err_total(d,o):\n",
    "    #d=np.array([[1,2,3],[4,7,6]],dtype=np.float32)\n",
    "    #o=np.array([[1,3,3],[4,5,6]],dtype=np.float32)\n",
    "    #print(err_total(d,o))\n",
    "    return np.sqrt(np.sum((d-o)**2))\n",
    "\n",
    "for i in range(epochs):\n",
    "    for xx,dd in zip(x,d):\n",
    "        print('----------')\n",
    "        #print('xx.shape:',xx.shape) #(n,)\n",
    "        #print('dd.shape:',dd.shape) #(l,)\n",
    "        \n",
    "        #正向传播计算\n",
    "        yy=f(np.dot(v,xx))  #[m x n] * [n x 1] => [m x 1] #m个隐层节点\n",
    "        oo=f(np.dot(w,yy))  #[l x m] *[m x 1]  => [l x 1] #l个输出节点\n",
    "        #print('yy.shape:',yy.shape) #(m,)\n",
    "        #print('oo.shape:',oo.shape) #(l,)\n",
    "        #print('v.shape:',v.shape)   #(n,m)\n",
    "        #print('w.shape:',w.shape)   #(l,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        #反向传播计算\n",
    "        #公式(3.25a)\n",
    "        #delta_o[k]=(dd[k]-oo[k])*(1-oo[k])*oo[k]\n",
    "        delta_o=(dd-oo)*(1-oo)*oo  #elementwise,计算每个输出节点的梯度值\n",
    "        #公式 （3.25b）\n",
    "        #np.dot(delta_o,w) => 隐层节点对应每个输出节点的梯度值的加权和\n",
    "        delta_y=np.dot(delta_o,w)*(1-yy)*yy   #np.dot(delta_o,w).shape=>(m,)\n",
    "        #print('delta_o.shape:',delta_o.shape) #(l,)\n",
    "        #print('delta_y.shape:',delta_y.shape) #(m,)\n",
    "\n",
    "        #更新权重\n",
    "        w+=lamda*np.dot(delta_o.reshape(l,1),yy.reshape(1,m))  #公式 (3.26a),^w[j,k]+=lambda*o[k]*y[j]\n",
    "        v+=lamda*np.dot(delta_y.reshape(m,1),xx.reshape(1,n))  #公式 (3.26b),^v[i,j]+=lambda*y[j]*x[i]\n",
    "        \n",
    "        #------------------\n",
    "        print(w.reshape(-1))\n",
    "        print(v.reshape(-1))\n",
    "        #print(w)\n",
    "        #print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
