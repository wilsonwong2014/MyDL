{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练\n",
    "\n",
    "### 模型\n",
    "<img src=\"model_data/fig_model2.png\" />\n",
    "\n",
    "###### def yolo_body(inputs, num_anchors, num_classes)\n",
    "    生成 yolo_body模型\n",
    "    模型输入:\n",
    "        yolo_model.input --- inputs,shape=>(batch_size,416,416,3)\n",
    "    模型输出:\n",
    "        yolo_model.output => [y1,y2,y3]\n",
    "        y1 --- shape=>(batch_size,13,13,255)=>(batch_size,13,13,3,25)\n",
    "        y2 --- shape=>(batch_size,26,26,255)=>(batch_size,26,26,3,25)\n",
    "        y3 --- shape=>(batch_size,52,52,255)=>(batch_size,52,52,3,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练执行流程线索\n",
    "\n",
    "    构建模型 create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/yolo_weights.h5')\n",
    "      构建模型 model = Model([model_body.input, *y_true], model_loss)\n",
    "          模型输入\n",
    "              输入数据 image_input = Input(shape=(None, None, 3))\n",
    "              标注数据 y_true => [y1,y2,y3]=> [(?,13,13,3,25),(?,26,26,3,25),(?,52,52,3,25)]\n",
    "          模型输出 model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "                    arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "                    [*model_body.output, *y_true])\n",
    "    编译模型\n",
    "    训练模型\n",
    "        数据生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#-----GPU配置-----\\n#  提示：在其他代码之前进行以下配置\\n\\n#禁用GPU\\nimport os\\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #-1:禁用,0-n启用第几块显卡，多个以逗号隔开\\n\\nimport tensorflow as tf\\nimport keras.backend.tensorflow_backend as KTF\\n\\n#ConfigProto配置\\nconfig = tf.ConfigProto()\\n\\n#设置GPU的百分比，程序需要还是会突破阈值\\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.8 #0-1之间的浮点数表示占用百分比\\n#GPU按需使用,不全部占满显存, 按需分配\\nconfig.gpu_options.allow_growth=True #True:按需分配,False:一次性满额分配\\n\\n# 设置session\\nsess = tf.Session(config=config)\\nKTF.set_session(sess)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#-----GPU配置-----\n",
    "#  提示：在其他代码之前进行以下配置\n",
    "\n",
    "#禁用GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #-1:禁用,0-n启用第几块显卡，多个以逗号隔开\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "#ConfigProto配置\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "#设置GPU的百分比，程序需要还是会突破阈值\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8 #0-1之间的浮点数表示占用百分比\n",
    "#GPU按需使用,不全部占满显存, 按需分配\n",
    "config.gpu_options.allow_growth=True #True:按需分配,False:一次性满额分配\n",
    "\n",
    "# 设置session\n",
    "sess = tf.Session(config=config)\n",
    "KTF.set_session(sess)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrain the YOLO model for your own dataset.\n",
    "\"\"\"\n",
    "import pdb\n",
    "#pdb.set_trace()\n",
    "import os\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from yolo3.utils import get_random_data\n",
    "\n",
    "path='%s/work/data/yolo/mytrain_model_data'%(os.getenv('HOME')) #数据存放根目录\n",
    "    \n",
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    '''加载检测类别名称\n",
    "    @param classes_path 检测类别文件,文件内容每行表示一个类别名称，如：\n",
    "        dog\n",
    "        cat\n",
    "    @return class_names [list]检测类别名称\n",
    "    '''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    '''加载锚点数组\n",
    "    @param anchors_path 锚点文件路径,文件内容如下所示：\n",
    "        10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "        每两个组成一个锚点，标识边框的宽度与高度wh\n",
    "    @return np.array(anchors).reshape(-1, 2) \n",
    "    '''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "\n",
    "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/yolo_weights.h5'):\n",
    "    '''create the training model'''\n",
    "    '''构建模型\n",
    "    @param input_shape     [tuple]模型输入尺寸,val=>(416,416),第一个为高度，第二个为宽度\n",
    "    @param anchors         [array]锚点数组,shape=>(9,2),第一列为宽度，第二列为高度\n",
    "    @param num_classes     [int  ]检测类别数,val=>20\n",
    "    @param load_pretrained [bool ]是否预加载参数\n",
    "    @param freeze_body     [int  ]模型层的冻结方式，1-冻结darknet53,2-除y1y2y3的所有层，其他-不做冻结设置\n",
    "    @param weights_path    [str  ]预训练模型参数路径\n",
    "\n",
    "    @return model\n",
    "        模型输入:image_data,y1,y2,y3\n",
    "        模型输出:loss\n",
    "    '''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3)) #实例化输入张量，shape不含batch_size\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    #=============定义输出标签值:y_true=[y1,y2,y3]=============\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
    "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
    "    '''\n",
    "    h=>416\n",
    "    w=>416\n",
    "    num_anchors=>9\n",
    "    num_classes=>20\n",
    "    y_true=>[<tf.Tensor 'input_1:0' shape=(?, 13, 13, 3, 25) dtype=float32>,\n",
    "             <tf.Tensor 'input_2:0' shape=(?, 26, 26, 3, 25) dtype=float32>,\n",
    "             <tf.Tensor 'input_3:0' shape=(?, 52, 52, 3, 25) dtype=float32>]\n",
    "    '''\n",
    "    \n",
    "    #=============yolo主体模型================================\n",
    "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
    "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    #=============加载预训练模型参数==========================\n",
    "    if load_pretrained and os.path.exists(weights_path):\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "            #for i,layer in enumerate(model_body.layers):\n",
    "            #    print('{:>3d}:{name}-{trainable}'.format(i,name=layer.name,trainable=layer.trainable))\n",
    "\n",
    "    #=============定义网络loss===============================\n",
    "    '''Lambda层\n",
    "        keras.layers.core.Lambda(function, output_shape=None, mask=None, arguments=None)\n",
    "    本函数用以对上一层的输出施以任何Theano/TensorFlow表达式\n",
    "    参数\n",
    "        function：要实现的函数，该函数仅接受一个变量，即上一层的输出\n",
    "        output_shape：函数应该返回的值的shape，可以是一个tuple，也可以是一个根据输入shape计算输出shape的函数\n",
    "        mask: 掩膜\n",
    "        arguments：可选，字典，用来记录向函数中传递的其他关键字参数    \n",
    "    '''\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "        [*model_body.output, *y_true])\n",
    "\n",
    "    #=============重构模型===================================\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    #输出最终训练模型\n",
    "    return model\n",
    "\n",
    "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/tiny_yolo_weights.h5'):\n",
    "    '''create the training model, for Tiny YOLOv3'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
    "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
    "\n",
    "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
    "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained and os.path.exists(weights_path):\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze the darknet body or freeze all but 2 output layers.\n",
    "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    '''数据生成器\n",
    "    @param annotations_lines 训练样本，每行形如以下格式：img_file 6,1,314,262,19 40,97,121,411,4 137,36,169,109,14 180,36,216,104,14 96,39,123,103,14 \n",
    "        Row format: image_file_path box1 box2 ... boxN;\n",
    "        Box format: x_min,y_min,x_max,y_max,class_id (no space).\n",
    "        范本：\n",
    "            path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3\n",
    "            path/to/img2.jpg 120,300,250,600,2    \n",
    "    @param batch_size 批大小,32\n",
    "    @param input_shape 模型输入尺寸(416,416)\n",
    "    @param anchors bounding box 锚点wh:[[10,13],  [16,30],  [33,23],  [30,61],  [62,45],  [59,119],  [116,90],  [156,198],  [373,326]]\n",
    "    @param num_classes 标签类别数,20\n",
    "    @return yield [image_data, *y_true], np.zeros(batch_size) => ([image_data,y1,y2,y3],loss)\n",
    "                                                              => ([(32,416,416,3),(32,13,13,3,25),(32,26,26,3,25),(32,52,52,3,25),(32,)])\n",
    "    '''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0: #一轮\n",
    "                np.random.shuffle(annotation_lines) #打乱排序\n",
    "            #生成一个样本数据,image.shape=>(416,416,3),box.shape=>(20,5)\n",
    "            #image做数据增强、归一化处理；box与image同步做变形、偏移处理，未归一化\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)  \n",
    "            image_data.append(image) #图像序列\n",
    "            box_data.append(box)     #bounding box序列\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)                                           #=>shape:(32,416,416,3)\n",
    "        box_data = np.array(box_data)                                               #=>shape:(32,20,5)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes) #=>把样本原始数据转换为训练需要的数据格式\n",
    "        '''\n",
    "        image_data:shape=>(32,416,416,3),模型输入图像数据\n",
    "        y_true:[y1,y2,y3],               模型输入Box标签数据\n",
    "            y1:shape=>(32,13,13,3,25)\n",
    "            y2:shape=>(32,26,26,3,25)\n",
    "            y3:shape=>(32,52,52,3,25)\n",
    "        np.zeros(batch_size):            模型输出loss\n",
    "\n",
    "        yield [image_data, *y_true], np.zeros(batch_size) => ([image_data,y1,y2,y3],loss)\n",
    "                                                          => ([(32,416,416,3),(32,13,13,3,25),(32,26,26,3,25),(32,52,52,3,25),(32,)])\n",
    "        '''        \n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "\n",
    "#_main() #训练函数入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============train_model================\n",
      "--------params--------\n",
      "path: /home/hjw/work/data/yolo/mytrain_model_data\n",
      "log_dir: /home/hjw/work/data/yolo/mytrain_model_data/log\n",
      "model_file: /home/hjw/work/data/yolo/mytrain_model_data/yolo_weights.h5\n",
      "cp_file: /home/hjw/work/data/yolo/mytrain_model_data/cp_file.h5\n",
      "samples_path: /home/hjw/work/data/yolo/mytrain_model_data/train.txt\n",
      "num_samples: 5717\n",
      "num_train: 5146\n",
      "num_val: 571\n",
      "classes_path: /home/hjw/work/data/yolo/mytrain_model_data/voc_classes.txt\n",
      "num_classes: 20\n",
      "anchors_path: /home/hjw/work/data/yolo/mytrain_model_data/yolo_anchors.txt\n",
      "num_anchors: 9\n",
      "is_pre_fine: True\n",
      "epochs_pre_fine: 1\n",
      "lr_pre_fine: 0.001\n",
      "lr_full_fine: 0.0001\n",
      "is_tiny_version: False\n",
      "input_shape: (416, 416)\n",
      "class_names: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
      "anchors: [[ 22.  33.]\n",
      " [ 46.  52.]\n",
      " [ 58. 113.]\n",
      " [106. 186.]\n",
      " [107.  76.]\n",
      " [188. 274.]\n",
      " [202. 132.]\n",
      " [374. 195.]\n",
      " [374. 337.]]\n",
      "\n",
      "\n",
      "create_model!\n",
      "Create YOLOv3 model with 9 anchors and 20 classes.\n",
      "Load weights /home/hjw/work/data/yolo/mytrain_model_data/yolo_weights.h5.\n",
      "Freeze the first 249 layers of total 252 layers.\n",
      "  0:input_1-False\n",
      "  1:conv2d_1-False\n",
      "  2:batch_normalization_1-False\n",
      "  3:leaky_re_lu_1-False\n",
      "  4:zero_padding2d_1-False\n",
      "  5:conv2d_2-False\n",
      "  6:batch_normalization_2-False\n",
      "  7:leaky_re_lu_2-False\n",
      "  8:conv2d_3-False\n",
      "  9:batch_normalization_3-False\n",
      " 10:leaky_re_lu_3-False\n",
      " 11:conv2d_4-False\n",
      " 12:batch_normalization_4-False\n",
      " 13:leaky_re_lu_4-False\n",
      " 14:add_1-False\n",
      " 15:zero_padding2d_2-False\n",
      " 16:conv2d_5-False\n",
      " 17:batch_normalization_5-False\n",
      " 18:leaky_re_lu_5-False\n",
      " 19:conv2d_6-False\n",
      " 20:batch_normalization_6-False\n",
      " 21:leaky_re_lu_6-False\n",
      " 22:conv2d_7-False\n",
      " 23:batch_normalization_7-False\n",
      " 24:leaky_re_lu_7-False\n",
      " 25:add_2-False\n",
      " 26:conv2d_8-False\n",
      " 27:batch_normalization_8-False\n",
      " 28:leaky_re_lu_8-False\n",
      " 29:conv2d_9-False\n",
      " 30:batch_normalization_9-False\n",
      " 31:leaky_re_lu_9-False\n",
      " 32:add_3-False\n",
      " 33:zero_padding2d_3-False\n",
      " 34:conv2d_10-False\n",
      " 35:batch_normalization_10-False\n",
      " 36:leaky_re_lu_10-False\n",
      " 37:conv2d_11-False\n",
      " 38:batch_normalization_11-False\n",
      " 39:leaky_re_lu_11-False\n",
      " 40:conv2d_12-False\n",
      " 41:batch_normalization_12-False\n",
      " 42:leaky_re_lu_12-False\n",
      " 43:add_4-False\n",
      " 44:conv2d_13-False\n",
      " 45:batch_normalization_13-False\n",
      " 46:leaky_re_lu_13-False\n",
      " 47:conv2d_14-False\n",
      " 48:batch_normalization_14-False\n",
      " 49:leaky_re_lu_14-False\n",
      " 50:add_5-False\n",
      " 51:conv2d_15-False\n",
      " 52:batch_normalization_15-False\n",
      " 53:leaky_re_lu_15-False\n",
      " 54:conv2d_16-False\n",
      " 55:batch_normalization_16-False\n",
      " 56:leaky_re_lu_16-False\n",
      " 57:add_6-False\n",
      " 58:conv2d_17-False\n",
      " 59:batch_normalization_17-False\n",
      " 60:leaky_re_lu_17-False\n",
      " 61:conv2d_18-False\n",
      " 62:batch_normalization_18-False\n",
      " 63:leaky_re_lu_18-False\n",
      " 64:add_7-False\n",
      " 65:conv2d_19-False\n",
      " 66:batch_normalization_19-False\n",
      " 67:leaky_re_lu_19-False\n",
      " 68:conv2d_20-False\n",
      " 69:batch_normalization_20-False\n",
      " 70:leaky_re_lu_20-False\n",
      " 71:add_8-False\n",
      " 72:conv2d_21-False\n",
      " 73:batch_normalization_21-False\n",
      " 74:leaky_re_lu_21-False\n",
      " 75:conv2d_22-False\n",
      " 76:batch_normalization_22-False\n",
      " 77:leaky_re_lu_22-False\n",
      " 78:add_9-False\n",
      " 79:conv2d_23-False\n",
      " 80:batch_normalization_23-False\n",
      " 81:leaky_re_lu_23-False\n",
      " 82:conv2d_24-False\n",
      " 83:batch_normalization_24-False\n",
      " 84:leaky_re_lu_24-False\n",
      " 85:add_10-False\n",
      " 86:conv2d_25-False\n",
      " 87:batch_normalization_25-False\n",
      " 88:leaky_re_lu_25-False\n",
      " 89:conv2d_26-False\n",
      " 90:batch_normalization_26-False\n",
      " 91:leaky_re_lu_26-False\n",
      " 92:add_11-False\n",
      " 93:zero_padding2d_4-False\n",
      " 94:conv2d_27-False\n",
      " 95:batch_normalization_27-False\n",
      " 96:leaky_re_lu_27-False\n",
      " 97:conv2d_28-False\n",
      " 98:batch_normalization_28-False\n",
      " 99:leaky_re_lu_28-False\n",
      "100:conv2d_29-False\n",
      "101:batch_normalization_29-False\n",
      "102:leaky_re_lu_29-False\n",
      "103:add_12-False\n",
      "104:conv2d_30-False\n",
      "105:batch_normalization_30-False\n",
      "106:leaky_re_lu_30-False\n",
      "107:conv2d_31-False\n",
      "108:batch_normalization_31-False\n",
      "109:leaky_re_lu_31-False\n",
      "110:add_13-False\n",
      "111:conv2d_32-False\n",
      "112:batch_normalization_32-False\n",
      "113:leaky_re_lu_32-False\n",
      "114:conv2d_33-False\n",
      "115:batch_normalization_33-False\n",
      "116:leaky_re_lu_33-False\n",
      "117:add_14-False\n",
      "118:conv2d_34-False\n",
      "119:batch_normalization_34-False\n",
      "120:leaky_re_lu_34-False\n",
      "121:conv2d_35-False\n",
      "122:batch_normalization_35-False\n",
      "123:leaky_re_lu_35-False\n",
      "124:add_15-False\n",
      "125:conv2d_36-False\n",
      "126:batch_normalization_36-False\n",
      "127:leaky_re_lu_36-False\n",
      "128:conv2d_37-False\n",
      "129:batch_normalization_37-False\n",
      "130:leaky_re_lu_37-False\n",
      "131:add_16-False\n",
      "132:conv2d_38-False\n",
      "133:batch_normalization_38-False\n",
      "134:leaky_re_lu_38-False\n",
      "135:conv2d_39-False\n",
      "136:batch_normalization_39-False\n",
      "137:leaky_re_lu_39-False\n",
      "138:add_17-False\n",
      "139:conv2d_40-False\n",
      "140:batch_normalization_40-False\n",
      "141:leaky_re_lu_40-False\n",
      "142:conv2d_41-False\n",
      "143:batch_normalization_41-False\n",
      "144:leaky_re_lu_41-False\n",
      "145:add_18-False\n",
      "146:conv2d_42-False\n",
      "147:batch_normalization_42-False\n",
      "148:leaky_re_lu_42-False\n",
      "149:conv2d_43-False\n",
      "150:batch_normalization_43-False\n",
      "151:leaky_re_lu_43-False\n",
      "152:add_19-False\n",
      "153:zero_padding2d_5-False\n",
      "154:conv2d_44-False\n",
      "155:batch_normalization_44-False\n",
      "156:leaky_re_lu_44-False\n",
      "157:conv2d_45-False\n",
      "158:batch_normalization_45-False\n",
      "159:leaky_re_lu_45-False\n",
      "160:conv2d_46-False\n",
      "161:batch_normalization_46-False\n",
      "162:leaky_re_lu_46-False\n",
      "163:add_20-False\n",
      "164:conv2d_47-False\n",
      "165:batch_normalization_47-False\n",
      "166:leaky_re_lu_47-False\n",
      "167:conv2d_48-False\n",
      "168:batch_normalization_48-False\n",
      "169:leaky_re_lu_48-False\n",
      "170:add_21-False\n",
      "171:conv2d_49-False\n",
      "172:batch_normalization_49-False\n",
      "173:leaky_re_lu_49-False\n",
      "174:conv2d_50-False\n",
      "175:batch_normalization_50-False\n",
      "176:leaky_re_lu_50-False\n",
      "177:add_22-False\n",
      "178:conv2d_51-False\n",
      "179:batch_normalization_51-False\n",
      "180:leaky_re_lu_51-False\n",
      "181:conv2d_52-False\n",
      "182:batch_normalization_52-False\n",
      "183:leaky_re_lu_52-False\n",
      "184:add_23-False\n",
      "185:conv2d_53-False\n",
      "186:batch_normalization_53-False\n",
      "187:leaky_re_lu_53-False\n",
      "188:conv2d_54-False\n",
      "189:batch_normalization_54-False\n",
      "190:leaky_re_lu_54-False\n",
      "191:conv2d_55-False\n",
      "192:batch_normalization_55-False\n",
      "193:leaky_re_lu_55-False\n",
      "194:conv2d_56-False\n",
      "195:batch_normalization_56-False\n",
      "196:leaky_re_lu_56-False\n",
      "197:conv2d_57-False\n",
      "198:batch_normalization_57-False\n",
      "199:leaky_re_lu_57-False\n",
      "200:conv2d_60-False\n",
      "201:batch_normalization_59-False\n",
      "202:leaky_re_lu_59-False\n",
      "203:up_sampling2d_1-False\n",
      "204:concatenate_1-False\n",
      "205:conv2d_61-False\n",
      "206:batch_normalization_60-False\n",
      "207:leaky_re_lu_60-False\n",
      "208:conv2d_62-False\n",
      "209:batch_normalization_61-False\n",
      "210:leaky_re_lu_61-False\n",
      "211:conv2d_63-False\n",
      "212:batch_normalization_62-False\n",
      "213:leaky_re_lu_62-False\n",
      "214:conv2d_64-False\n",
      "215:batch_normalization_63-False\n",
      "216:leaky_re_lu_63-False\n",
      "217:conv2d_65-False\n",
      "218:batch_normalization_64-False\n",
      "219:leaky_re_lu_64-False\n",
      "220:conv2d_68-False\n",
      "221:batch_normalization_66-False\n",
      "222:leaky_re_lu_66-False\n",
      "223:up_sampling2d_2-False\n",
      "224:concatenate_2-False\n",
      "225:conv2d_69-False\n",
      "226:batch_normalization_67-False\n",
      "227:leaky_re_lu_67-False\n",
      "228:conv2d_70-False\n",
      "229:batch_normalization_68-False\n",
      "230:leaky_re_lu_68-False\n",
      "231:conv2d_71-False\n",
      "232:batch_normalization_69-False\n",
      "233:leaky_re_lu_69-False\n",
      "234:conv2d_72-False\n",
      "235:batch_normalization_70-False\n",
      "236:leaky_re_lu_70-False\n",
      "237:conv2d_73-False\n",
      "238:batch_normalization_71-False\n",
      "239:leaky_re_lu_71-False\n",
      "240:conv2d_58-False\n",
      "241:conv2d_66-False\n",
      "242:conv2d_74-False\n",
      "243:batch_normalization_58-False\n",
      "244:batch_normalization_65-False\n",
      "245:batch_normalization_72-False\n",
      "246:leaky_re_lu_58-False\n",
      "247:leaky_re_lu_65-False\n",
      "248:leaky_re_lu_72-False\n",
      "249:conv2d_59-True\n",
      "250:conv2d_67-True\n",
      "251:conv2d_75-True\n",
      "==========yolo_loss===========\n",
      "mdoel.input_shape: [(None, None, None, 3), (None, 13, 13, 3, 25), (None, 26, 26, 3, 25), (None, 52, 52, 3, 25)]\n",
      "mdoel.output_shape: (None, 1)\n",
      "Train on 5146 samples, val on 571 samples, with batch size 14.\n",
      "<class 'generator'>\n",
      "Epoch 1/1\n",
      "367/367 [==============================] - 283s 770ms/step - loss: 42.7316 - acc: 0.0000e+00 - val_loss: 42.9316 - val_acc: 0.0000e+00\n",
      "unfreeze from start_layer: 0\n",
      "Train on 5146 samples, val on 571 samples, with batch size 14.\n",
      "<class 'generator'>\n",
      "Epoch 2/2\n",
      "367/367 [==============================] - 290s 790ms/step - loss: 40.7453 - acc: 0.0000e+00 - val_loss: 39.6051 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to file: /home/hjw/work/data/yolo/mytrain_model_data/yolo_weights.h5\n"
     ]
    }
   ],
   "source": [
    "print('===============train_model================')\n",
    "path='%s/work/data/yolo/mytrain_model_data'%(os.getenv('HOME')) #数据存放根目录\n",
    "\n",
    "#===============训练参数配置==================\n",
    "samples_path = '%s/train.txt'%(path)         #训练样本，注释文件,格式：/img_file 6,1,314,262,19 40,97,121,411,4 137,36,169,109,14 180,36,216,104,14 96,39,123,103,14\n",
    "                                             #由voc_annotation.py生成\n",
    "log_dir = '%s/log'%(path)                    #日志目录\n",
    "classes_path = '%s/voc_classes.txt'%(path)   #VOC数据集标签类别(20类)\n",
    "anchors_path = '%s/yolo_anchors.txt'%(path)  #yolo anchor配置文件[10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326]\n",
    "class_names = get_classes(classes_path)      #读取VOC数据集标签类别[aeroplane,bicycle,bird,boat,bottle,bus,car,cat,chair,cow,diningtable,dog,horse,motorbike,person,pottedplant,sheep,sofa,train,tvmonitor]\n",
    "num_classes = len(class_names)               #标签类别数(20)\n",
    "anchors = get_anchors(anchors_path)          #锚点wh,anchors [[10,13],  [16,30],  [33,23],  [30,61],  [62,45],  [59,119],  [116,90],  [156,198],  [373,326]]\n",
    "num_anchors = len(anchors)                   #锚框个数\n",
    "input_shape = (416,416)                      # multiple of 32, hw #模型输入尺寸\n",
    "\n",
    "#--------------加载训练样本文件---------------\n",
    "val_split = 0.1                     #留出百分之10的数据用于校验\n",
    "is_pre_fine = True                  #是否预调最后y1,y2,y3参数，首次运行调参设置为True\n",
    "epochs_pre_fine=1                   #首次预调参数迭代次数\n",
    "lr_pre_fine =1e-3                   #首次预调参数学习率\n",
    "epochs_full_fine=2                  #全局调参迭代次数\n",
    "lr_full_fine=1e-4                   #全局调参学习率\n",
    "start_layer=0                       #全局调参起始层\n",
    "batch_size=14                       #批大小\n",
    "with open(samples_path) as f:\n",
    "    lines = f.readlines()           #每行形如以下格式：img_file 6,1,314,262,19 40,97,121,411,4 137,36,169,109,14 180,36,216,104,14 96,39,123,103,14\n",
    "np.random.seed(10101)               #设置随机种子，固化每次的随机序列(伪随机)\n",
    "np.random.shuffle(lines)            #打乱顺序\n",
    "np.random.seed(None)                #恢复随机状态\n",
    "num_samples = len(lines)            #样本集大小\n",
    "num_val = int(len(lines)*val_split) #校验样本数\n",
    "num_train = len(lines) - num_val    #训练样本数\n",
    "\n",
    "is_tiny_version = len(anchors)==6 # default setting\n",
    "if is_tiny_version:\n",
    "    model_file='%s/tiny_yolo_weights.h5'%path\n",
    "    cp_file='%s/tiny_cp_file.h5'%path\n",
    "else:\n",
    "    model_file='%s/yolo_weights.h5'%path\n",
    "    cp_file='%s/cp_file.h5'%path\n",
    "\n",
    "print('--------params--------')\n",
    "print('path:',path)\n",
    "print('log_dir:',log_dir)\n",
    "print('model_file:',model_file)\n",
    "print('cp_file:',cp_file)\n",
    "print('samples_path:',samples_path)\n",
    "print('num_samples:',num_samples)\n",
    "print('num_train:',num_train)\n",
    "print('num_val:',num_val)\n",
    "print('classes_path:',classes_path)\n",
    "print('num_classes:',num_classes)\n",
    "print('anchors_path:',anchors_path)\n",
    "print('num_anchors:',len(anchors))\n",
    "print('is_pre_fine:',is_pre_fine)\n",
    "print('epochs_pre_fine:',epochs_pre_fine)\n",
    "print('lr_pre_fine:',lr_pre_fine)\n",
    "print('lr_full_fine:',lr_full_fine)\n",
    "print('is_tiny_version:',is_tiny_version)\n",
    "print('input_shape:',input_shape)\n",
    "print('class_names:',class_names)\n",
    "print('anchors:',anchors)\n",
    "print('\\n')\n",
    "\n",
    "#================训练回调函数设置=================\n",
    "#TensorBoard可视化日志\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "#断点保存\n",
    "checkpoint = ModelCheckpoint(cp_file,\n",
    "    monitor='val_loss', save_weights_only=True, save_best_only=True, period=1)\n",
    "#学习率\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "#退出条件\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "#===============创建训练模型=================\n",
    "#freeze_body=2 => 冻结除y1,y2,y3的所有层\n",
    "if is_tiny_version:\n",
    "    print('create_tiny_model!')\n",
    "    model = create_tiny_model(input_shape, anchors, num_classes,\n",
    "        freeze_body=2, weights_path=model_file)\n",
    "else:\n",
    "    print('create_model!')\n",
    "    #freeze_body:1-解冻所有层，2-冻结darknet\n",
    "    model = create_model(input_shape, anchors, num_classes,\n",
    "        freeze_body=2, weights_path=model_file) # make sure you know what you freeze    \n",
    "    print('mdoel.input_shape:',model.input_shape)   #mdoel.input_shape: [(None, None, None, 3), (None, 13, 13, 3, 25), (None, 26, 26, 3, 25), (None, 52, 52, 3, 25)]\n",
    "    print('mdoel.output_shape:',model.output_shape) #mdoel.output_shape: (None, 1) --- loss\n",
    "\n",
    "#=================预调参数==================\n",
    "if is_pre_fine:\n",
    "    #-----------------模型编译------------------\n",
    "    model.compile(optimizer=Adam(lr=lr_pre_fine), loss={'yolo_loss': lambda y_true, y_pred: y_pred},metrics=['accuracy'])\n",
    "\n",
    "    #-----------------构造训练数据生成器------------------\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    #anchors=>[[10,13],  [16,30],  [33,23],  [30,61],  [62,45],  [59,119],  [116,90],  [156,198],  [373,326]]\n",
    "    data_gen=data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes)\n",
    "    print(type(data_gen))\n",
    "    #print('data_gen.shape:',data_gen.shape) #\n",
    "    #data_gen.shape=>([(32, 416, 416, 3), (32, 13, 13, 3, 25), (32, 26, 26, 3, 25), (32, 52, 52, 3, 25)],(32,))\n",
    "\n",
    "    #-----------------模型训练----------------------------\n",
    "    history=model.fit_generator(data_gen,\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=epochs_pre_fine,#epochs=50,\n",
    "            initial_epoch=0,\n",
    "            callbacks=[logging, early_stopping, checkpoint])\n",
    "\n",
    "#=================全面调参==================\n",
    "if True:\n",
    "    #-------------解冻层-----------------\n",
    "    if start_layer>=0:\n",
    "        print('unfreeze from start_layer:',start_layer)\n",
    "        for i in range(start_layer,len(model.layers)):\n",
    "            model.layers[i].trainable = True\n",
    "\n",
    "    #-------------模型编译----------------\n",
    "    model.compile(optimizer=Adam(lr=lr_full_fine), loss={'yolo_loss': lambda y_true, y_pred: y_pred},metrics=['accuracy'])\n",
    "\n",
    "    #-------------构造训练数据生成器--------\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    #anchors=>[[10,13],  [16,30],  [33,23],  [30,61],  [62,45],  [59,119],  [116,90],  [156,198],  [373,326]]\n",
    "    data_gen=data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes)\n",
    "    print(type(data_gen))\n",
    "    #print('data_gen.shape:',data_gen.shape) #\n",
    "    #data_gen.shape=>([(32, 416, 416, 3), (32, 13, 13, 3, 25), (32, 26, 26, 3, 25), (32, 52, 52, 3, 25)],(32,))\n",
    "\n",
    "    #-------------模型训练----------------\n",
    "    history=model.fit_generator(data_gen,\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=epochs_full_fine,#epochs=50,\n",
    "            initial_epoch=epochs_pre_fine,\n",
    "            callbacks=[logging,early_stopping, checkpoint])\n",
    "\n",
    "#=================保存训练参数================\n",
    "print('save model to file:',model_file)\n",
    "model.save_weights(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
